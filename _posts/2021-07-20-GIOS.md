---
layout: post
title:  "GIOS Retrospective"
date:   "2021-06-07" 
#categories: jekyll update
colorspace: red
published: true
---

## Introduction

This is the first of ten classes I chose to take for my masters in computer science, because a bachelor's wasn't enough pain, or maybe my hubris was too big for my britches.
I had a brief run in a class the previous semester; IIS, however the first assignment had some issues with portability and I scored a zero, thankfully it was before the drop deadline, but I learned my lesson as to not take any graduate level computer science class lightly.

At the time of me writing this, I finished the class about 3 months ago and received an A for my efforts, despite doing about average on all exams and not finishing a portion of the first assignment, a decision that I made after spending about fifty hours in a week, due to lack of experience with C or any low level networking APIs. 


## Assignments

### Project 1: Fileserver in C
This project is absolutely serves a litmus test for not only this particular class but in my opinion the **entire program**. 

If you have no C programming experience there is going to be a big ramping up period for anyone attempting to do this project, but since it is in three different parts each with two separate files this is honestly a huge assignment. 

This project easily took me **50** man hours, and I didn't even do the last out of the 6 components. I kind of wish this was separated into two projects. But it's amazing exposure to C, Networking APIs, and multi-threaded programming.

The multi-threaded programming world can have some intimidating vocabulary, but the impetus of the entire abstraction is that resources can be dangerous to use across threads.

```
Mutex - A LOCK that you can put on any resource. 

Condition Variable - Toggles locks waiting on other processes

Race Condition - Behavior that changes based off timing, often when thread behavior is not defined correctly

Deadlock - Caused by two resources waiting for each's others locked resources, neither can proceed while the other has it's resource

Critical Section - Part of the program which accesses those shared resources
```

### Project 3: Cached File Server
This project is where things get real, everything that was needed in the last project is needed here even more so. Essentially a similar architecture to Apache, it's a multi process get file server/client with a shared memory buffer for serving cached files. It's the student's job to create the cache process, the server proxy, and the communication between the two.  

Before any content more multi-threaded vocabulary

```
IPC - Interprocess Communication

SHM(Shared Memory) - Some part of memory that sits outside both of two processes memory, essentially a file(DON'T GET ANY FUNNY IDEAS) but meant for IPC, in POSIX, it's named :D

MQUEUE(Message Queue) - A shared process Queue that's synchronization is handled by the OS in POSIX - named, that's nice isn't it

Semaphore - In normal english this word means a system of visual signaling by two flags held one in each hand
and in computer science it means this exactly, well they really are just shared integers, but they are typically used in a red light green light system, and will be addressed below as so.  

```

An interesting tidbit here that I had never forayed into was signal handlers. When establishing Posix shared memory and message queues they both become valid file descriptors found in `/dev/shm` and `/dev/mqueue` respectively. So when you tear down the process, you still have those resources active on your OS, which can of course cause problems when you try and reestablish those resources. But you pre-empt those **Signal Handlers** and tear down those resources before the process you are running. 

{% highlight C %}
static void _sig_handler(int signo)
{
	if (signo == SIGTERM || signo == SIGINT)
	{
		mqCleanup(QUEUE_NAME);
		exit(signo);
	}
}
{% endhighlight %}

Now for the meat, well if you can tell from earlier I was pretty happy that the shared memory and mqueue was named, and for good reason as it allows the processes to subscribe to the shared memory much easier. As you can see your shm names to a static constant rather than trying to search for them at run time.

The assignment requires students to support several inputs: nsegments, nsize, and nworkerthreads; number of shared memory segments created, the size of each segment, and the number of threads created by webproxy, and cache process respectively. The idea being that your architecture can scale to be quicker with more resources allocated to it.

The proxy creates the shared memory, and if you cleverly put those shared memory segment pointers, into a queue and lock it behind a mutex, you have a synchronized (atleast within the proxy process) shared memory. You can then use this later to share any info you want with the proxy's cache request threads and the cache's request handler's threads.

The proxy also establishes communication with the POSIX message queue and passes the memory location of it to threads so each can independently send messages to the queue. Which thanks to POSIX goodness, is an automatically synchronized data structure which we can run across processes/threads. Thanks OS folks. 
 
Meanwhile, the cache has established the messaged POSIX message queue, sent the worker threads it's info, and spun up all hte cache request request handler threads.

Here is where the fun IPC starts.

The proxy handler threads are looking for that mutex on the queue of requests, and if they manage to unlock it, it's now that they can retrieve a request from that queue. From here they have all the info they need to put a call into the message queue. They have a shared memory memory segment,name of the POSIX read write semaphore,and a request to a file. The message includes the **name** of the shared memory segment, and the all the information about the file request, simply in a string separated by spaces. Then it <span style="color:red"> **red lights** </span> setting the read semaphore to -1.

The cache's request handler's are looking at the POSIX message queue waiting for something to populate. When one receives the message they start going. It immediately checks it's cache using the information from the message queue string, and subscribes to the name of the shared memory segment, and the POSIX semaphore, using the string it got from the message queue for all further communication! It sends back over header info for the file, which includes file length and other info about the file. Then it <span style="color:green"> **green lights** </span> setting that read semaphore back to 0 using sem_post. And <span style="color:red"> **red lights** </span> the write semaphore.

The proxy handler then has full access to the SHM segment! It can now see if the cache has that file info, and if it does, it looks at the file length. 
We could just use that single established SHM but that wouldn't be performant would it?
Sidenote: if the cache doesn't have the request, we end this transaction here.

Should we just grab the total number of shm segments available?
Well okay what if all other segs are being used? Should we just wait or continue to use the single one we have?

If we wait for more segments indefinitely we could deadlock, where n threads trying to get more segments, all unable to do so, so spin lock won't work, we need a pass lock where if threads are unable to acquire the lock they move on.

{% highlight C %}
if pthread_mutex_trylock(mq_mutex)==0
{
	acquire_more_shm()
}
else {
	proceed_with_single_shm()
}
{% endhighlight %}

Well okay how many segments should we acquire? We could just grab as many as possible, but we could overshoot our file size, so we can get the minimum amount of segments required in order to transfer the file or as many segments as are available. 

`segs_to_acquire = min(int(FILESIZE/SEGSIZE),size(queue))`

But, because segment size is variable, we can't assume that our N branded messages are able to fit in the shared memory segment for the cache handler to properly use them all. 

`segs_to_acquire = min(NUMBER OF SEGMENTS TO ACQUIRE,int(segsize/msgesize))`

Once the segs are acquired the same message used to initially create contact between cache request handler and proxy request, can be sent through the already established shared memory channel.

And finally we can <span style="color:green"> **green light** </span> the writer semaphore. And <span style="color:red"> **red light** </span>

The cache handler finally can go to town after setting up all the other shms that the request sends it.

It loads the file in order of received shm's the file buffer, and <span style="color:green"> **green lights** </span> the reader semaphore after it's done.
And <span style="color:red"> **red lights** </span> the write semaphore if all the segments are full. 

It does this ad infinitum until the entire file has been read.

If we wanted to get even faster we could allocate each semaphore of the segment individually and ping them as they got done writing.
A constant ping pong back and forth.

Once we get the file done we reallocate the segments back into queue. Ta. Da. File served across processes, scalable.

## Lectures

I hope you like Elves, the lectures are chalk full of them.
Ada's teaching style is really quite good and the lectures can be extremely dense and require multiple watches to ingest in any meaningful way, it's unfortunate that I felt like I didn't have time didn't feel like committing the time to fully absorb the material especially towards the latter half of the class.


## Takeaways

We stand on the shoulders of many specialized builders

Abstraction is how humans make progress

There are no win-win design situations, only tradeoffs where the downsides are ignored

## Rating

This is the type of class that this program is built for, slightly masochistic nerds or people with inferiority complexes that have something to prove.